{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speech Command Recognition using CNN\n",
    "# Author: Claude\n",
    "# Date: November 2024\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import librosa\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Loading and Preprocessing Functions\n",
    "\n",
    "def load_audio_file(file_path, duration=1.0, sr=16000):\n",
    "    \"\"\"\n",
    "    Load and preprocess audio file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file with a fixed duration\n",
    "        y, sr = librosa.load(file_path, duration=duration, sr=sr)\n",
    "        \n",
    "        # Pad or truncate to ensure fixed length\n",
    "        if len(y) > sr:\n",
    "            y = y[:sr]\n",
    "        else:\n",
    "            y = np.pad(y, (0, max(0, sr - len(y))))\n",
    "            \n",
    "        return y, sr\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(y, sr, n_mfcc=13, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract MFCC features from audio signal\n",
    "    \"\"\"\n",
    "    try:\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, \n",
    "                                   hop_length=hop_length)\n",
    "        # Normalize MFCC\n",
    "        mfcc = (mfcc - np.mean(mfcc)) / np.std(mfcc)\n",
    "        return mfcc\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(y, sr):\n",
    "    \"\"\"\n",
    "    Apply random augmentation to audio signal\n",
    "    \"\"\"\n",
    "    # Random time shift\n",
    "    shift = np.random.randint(-sr//10, sr//10)\n",
    "    y_shifted = np.roll(y, shift)\n",
    "    if shift > 0:\n",
    "        y_shifted[:shift] = 0\n",
    "    else:\n",
    "        y_shifted[shift:] = 0\n",
    "    \n",
    "    # Random pitch shift\n",
    "    n_steps = np.random.randint(-4, 4)\n",
    "    y_pitched = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)\n",
    "    \n",
    "    # Add random noise\n",
    "    noise_factor = np.random.uniform(0, 0.01)\n",
    "    noise = np.random.normal(0, 1, len(y))\n",
    "    y_noisy = y + noise_factor * noise\n",
    "    \n",
    "    # Randomly choose one augmentation\n",
    "    augmented = np.random.choice([y_shifted, y_pitched, y_noisy])\n",
    "    return augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data_path, classes):\n",
    "    \"\"\"\n",
    "    Prepare dataset with features and labels\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        for file_name in os.listdir(class_path):\n",
    "            file_path = os.path.join(class_path, file_name)\n",
    "            \n",
    "            # Load and preprocess audio\n",
    "            y, sr = load_audio_file(file_path)\n",
    "            if y is None:\n",
    "                continue\n",
    "                \n",
    "            # Extract MFCC\n",
    "            mfcc = extract_mfcc(y, sr)\n",
    "            if mfcc is None:\n",
    "                continue\n",
    "                \n",
    "            features.append(mfcc)\n",
    "            labels.append(idx)\n",
    "            \n",
    "            # Add augmented version\n",
    "            if np.random.random() < 0.3:  # 30% chance of augmentation\n",
    "                y_aug = augment_audio(y, sr)\n",
    "                mfcc_aug = extract_mfcc(y_aug, sr)\n",
    "                if mfcc_aug is not None:\n",
    "                    features.append(mfcc_aug)\n",
    "                    labels.append(idx)\n",
    "    \n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    # Resize features to target shape (64, 64)\n",
    "    features_resized = []\n",
    "    for feature in features:\n",
    "        resized = tf.image.resize(feature[..., np.newaxis], [64, 64]).numpy()\n",
    "        features_resized.append(resized)\n",
    "    \n",
    "    features = np.array(features_resized)\n",
    "    \n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Model Definition\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create CNN model for speech recognition\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First Convolutional Block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        \n",
    "        # Dense Layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate scheduler\n",
    "def lr_schedule(epoch):\n",
    "    initial_lr = 0.001\n",
    "    if epoch < 10:\n",
    "        return initial_lr\n",
    "    else:\n",
    "        return initial_lr * tf.math.exp(-0.1 * (epoch - 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Training and Evaluation Functions\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, batch_size=32, epochs=50):\n",
    "    \"\"\"\n",
    "    Train the model with early stopping and learning rate scheduling\n",
    "    \"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        LearningRateScheduler(lr_schedule)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'])\n",
    "    ax1.plot(history.history['val_accuracy'])\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.legend(['Train', 'Validation'])\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'])\n",
    "    ax2.plot(history.history['val_loss'])\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Define parameters\n",
    "    data_path = \"path_to_dataset\"  # Update with actual path\n",
    "    classes = ['zero', 'one']  # Update with actual classes\n",
    "    \n",
    "    # Prepare dataset\n",
    "    print(\"Preparing dataset...\")\n",
    "    features, labels = prepare_dataset(data_path, classes)\n",
    "    \n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and compile model\n",
    "    print(\"Creating model...\")\n",
    "    model = create_cnn_model(input_shape=(64, 64, 1), num_classes=len(classes))\n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='sparse_categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    history = train_model(model, X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Generate predictions\n",
    "    y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "    \n",
    "    # Plot results\n",
    "    plot_training_history(history)\n",
    "    plot_confusion_matrix(y_test, y_pred, classes)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
